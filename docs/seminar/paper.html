<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
           "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="GENERATOR" content="TtH 4.03" />
 <style type="text/css"> div.p { margin-top: 7pt;}</style>
 <style type="text/css"><!--
 td div.comp { margin-top: -0.6ex; margin-bottom: -1ex;}
 td div.comb { margin-top: -0.6ex; margin-bottom: -.6ex;}
 td div.hrcomp { line-height: 0.9; margin-top: -0.8ex; margin-bottom: -1ex;}
 td div.norm {line-height:normal;}
 span.roman {font-family: serif; font-style: normal; font-weight: normal;} 
 span.overacc2 {position: relative;  left: .8em; top: -1.2ex;}
 span.overacc1 {position: relative;  left: .6em; top: -1.2ex;} --></style>
 <style type="text/css"><!--
 .tiny {font-size:30%;}
 .scriptsize {font-size:xx-small;}
 .footnotesize {font-size:x-small;}
 .smaller {font-size:smaller;}
 .small {font-size:small;}
 .normalsize {font-size:medium;}
 .large {font-size:large;}
 .larger {font-size:x-large;}
 .largerstill {font-size:xx-large;}
 .huge {font-size:300%;}
 --></style>

              
<title> Applying Learning To Rank and Document Aggregation Techniques on a
Translator-Expert Retrieval Framework </title>
</head>
<body><div>
 
<h1 align="center">Applying Learning To Rank and Document Aggregation Techniques on a
Translator-Expert Retrieval Framework </h1>

<div class="p"><!----></div>

<h3 align="center">

,


 </h3>

<div class="p"><!----></div>

<h2> Abstract</h2>
Expertise Retrieval, whose task is to suggest people with relevant expertise regarding to a query, has received increasing interest in recent years. In this paper, we propose an Expert Retrieval platform narrowed down to translators as experts. Issues and obstacles during design and development as well as acquired solutions and results are reported. Ranking the translators using Learning to Rank as well as documents' aggregation functions are two main issues which are specifically studied in the paper.

<div class="p"><!----></div>
 <a id="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Introduction</h2>
<a id="sec:introduction">
</a>
The goal of expertise retrieval is to link humans to expertise areas, and vice versa. In other words, the task of expertise retrieval is to identify a set of persons with relevant expertise for the given query [<a href="#er" id="CITEer">, </a>,<a href="#er-community-aware" id="CITEer-community-aware">, </a>].

<div class="p"><!----></div>
With the development of information retrieval (IR) techniques, many research efforts in this field have been made to address high-level IR and not just traditional document retrieval, such as entity retrieval and expertise retrieval [<a href="#er-sparse" id="CITEer-sparse">, </a>]. The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects [<a href="#trec2005" id="CITEtrec2005">, </a>,<a href="#er-community-aware">, </a>].

<div class="p"><!----></div>
Two principal approaches are proposed by <a href="#trec2005"> []</a> based on probabilistic language modeling techniques. They were formalized as so-called candidate models and document models. The candidate-based approach, also referred to as profile-based method, builds a textual representation of candidate experts and then ranks them based on the query. The document models first find documents relevant to the topic and then locate the experts associated with these documents [<a href="#er">, </a>].

<div class="p"><!----></div>
Ranking techniques are mostly one of the essential parts of IR frameworks. In recent years, Learning to Rank (L2R) has been studied extensively specially for document retrieval. It refers to machine learning techniques for training the model in a ranking task [<a href="#er">, </a>]. In essence, expert search is a ranking problem and thus the existing L2R techniques can be naturally applied to it [<a href="#l2r-intro" id="CITEl2r-intro">, </a>].

<div class="p"><!----></div>
As well as ranking techniques, aggregation functions have a significant effect on the performance of IR systems. Aggregate tasks are those where documents' similarities are not
the final outcome, but instead an intermediary component. In expert search, a ranking of candidate persons with relevant expertise to a query is generated after aggregation of their related documents [<a href="#agg-learning" id="CITEagg-learning">, </a>].

<div class="p"><!----></div>
This paper addresses the problem of searching translators as experts. We have applied Learning to Rank in a candidate-based approach. Additionally, different aggregation algorithms related to documents of translators have been studied.

<div class="p"><!----></div>
The remaining of the paper is organized as follows. In Section <a href="#sec:casestudy">II</a>, the Translator-Expert Retrieval framework is described in detail. Then, Section <a href="#sec:methods">III</a> explains methods used in the study. In Section <a href="#sec:apply">IV</a>, we report the result of applied methods on the framework. Finally, we conclude the study in Section <a href="#sec:conclusion">V</a>.


 <a id="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Case Study</h2>
<a id="sec:casestudy">
</a>
The data flow of expert searching is depicted in Figure<a href="#fig:architecture">1</a>. The client submits a document and searches for translators with a specific target language. Based on query document, the framework figures out offered price, delivery time, proficiency of translators and number of cooperation times related to each translator. The result is processed by a ranking system and the most related translators are offered to the client.

<div class="p"><!----></div>
As it is shown in Figure <a href="#fig:architecture">1</a>, the essential components of the framework are <i>Ranking</i>, <i>Proficiency Estimator</i>, <i>Scheduler</i> and <i>Profiler</i>.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a id="tth_fIg1">
</a> 
<div style="text-align:center"><img src="figures/dataflow.jpg" alt="figures/dataflow.jpg" />

<div style="text-align:center">Figure 1: System Data Flow
<a id="fig:architecture">
</a></div>
</div>

<div class="p"><!----></div>
<i>Ranking</i> system uses L2R to return the most related translators to the client. The training data is provided by a group of evaluators who are familiar with business of company using an evaluation system. The evaluation suggests three translators and by comparing between their factors, the evaluators choose one. In order to prevent bias in evaluation, the translators are suggested randomly and without name and picture. Applied learning to rank methods and results are described in Section <a href="#sec:methods">III</a>. <i>Proficiency Estimator</i> stores the previous-translated documents in the cloud and indexes them using Lucene library. The similarity between query and indexed documents is used as a base for estimation of translator's proficiency. In order to find proficiency value, the similarity scores are aggregated. The applied aggregation function is described in Section <a href="#sec:apply">IV</a>. <i>Scheduler</i> system figures out the delivery time based on timetable of translators. The scheduler builds a special data structure to calculate the response time. The detail of the process is out of scope of the paper. At last, <i>Profiler</i> accumulates personal information, translators' preferences as well as offered price and translation duration per word.

<div class="p"><!----></div>
Beside translator, a proofreader selected by the client revises the final translation. As well as reviewing, the proofreader assesses the quality of translation from different points of view (grammar, style, accuracy, content and language). The assessment is defined as a value between 1 (very bad) and 5 (perfect). As it is discussed in Section <a href="#sec:apply">IV</a>, it is used to evaluate aggregation algorithms for translator's proficiency.


 <a id="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Methods</h2>
<a id="sec:methods">
</a>
In this section, we study different methods and algorithms regarding to Aggregation Functions and Learning to Rank.

<div class="p"><!----></div>
     <a id="tth_sEc3.1"></a><h3>
3.1&nbsp;&nbsp;Aggregation Functions</h3>
The aggregation function has a significant impact on the performance of Expert Retrieval system. As a usual scenario in expert retrieval systems, first each document related to an expert is scored and ranked regarding to query. Then, the top N document scores associated with a candidate expert are aggregated in order to rank the experts. 

<div class="p"><!----></div>
The effect of different features on aggregation function is studied in <a href="#agg-gp2" id="CITEagg-gp2"> []</a>. As it is shown, number of documents is tightly related such that the performance of different queries are optimal for different values of N. Comparing query-based features using statistical measures, it inferred that the features may not, in general, be able to predict the optimal number of documents to aggregate for each query. Individual Expert Features is discussed in the next step. It is shown that relevant experts are associated with a higher ranked document than non-relevant experts. More interestingly, relevant experts are associated with less documents on average.

<div class="p"><!----></div>
<a href="#agg-vote" id="CITEagg-vote"> []</a> looks to expert search as a voting problem, where documents vote for the candidates with relevant expertise. Eleven data fusion as well as three statistically different document weighting were tested. In practice, the approach considers both number of documents and expert features regarding to the ranking score of the documents. The results show that while some of adapted voting techniques are most likely outperform others, the proposed approach is effective when using appropriate one.

<div class="p"><!----></div>
Later on, focusing on related features discussed before <a href="#agg-gp2"> []</a> introduces a new aggregation method. It uses genetic programming to learn a formula for the weights of document associations within the candidate profiles. The formula denoted as GP2 is as follows:

<div class="p"><!----></div>
GP2 = /(


<div class="p"><!----></div>
 where R is the rank of the document in the initial ranking and no_docs<sub>x</sub><sub>i</sub> is the total number of documents associated with expert x<sub>i</sub>.

<div class="p"><!----></div>
     <a id="tth_sEc3.2"></a><h3>
3.2&nbsp;&nbsp;Learning To Rank</h3>
Learning to rank refers to machine learning techniques for training the model in a ranking task. Due to its importance, learning to rank has been drawing broad attention in the machine learning community recently. 

<div class="p"><!----></div>
In learning to rank approach, the ranking problem is transformed to classification, regression and ordinal classification, and existing methods and techniques for solving machine learning problems are applied. As <a href="#l2r-intro"> []</a> points out the relation between learning to rank and ordinal classification, in ranking, one cares more about accurate ordering of objects, while in ordinal classification, one cares more about accurate ordered-categorization of objects.

<div class="p"><!----></div>
One primitive step in accumulating data required for learning to rank, is relevance judgment done by human annotators.  introduces three main strategies as follows.

<ul>
<li> <i>Relevance degree</i>: It specifies whether an object is relevant or not to the query. It can be either in binary judgment or by specifying the degree of relevance (e.g., Perfect, Excellent, Good, Fair, or Bad).
<div class="p"><!----></div>
</li>

<li> <i>Pairwise preference</i>: A pair of objects are compared in order to specify which one is more relevant with regards to a query.
<div class="p"><!----></div>
</li>

<li> <i>Total order</i>: The total order of all object with respect to a query is specified.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Among the three mentioned kinds of judgments, since the first is the easiest to obtain, it is the most popularly used judgment. While the third kind is more accurate but tedious for human annotators.

<div class="p"><!----></div>
The learning to rank techniques are categorized in three main groups: <i>Pointwise</i>, <i>Pairwise</i> and <i>Listwise</i>.

<div class="p"><!----></div>
In the pointwise approach, the ranking problem is transformed to classification, regression or ordinal classification. Therefore, the group structure of ranking is ignored in this approach [<a href="#l2r-intro">, </a>].

<div class="p"><!----></div>
The pairwise approach transforms the ranking problem into pairwise classification or regression. In fact, it cares about the relative order between two documents. In the pairwise approach, the group structure of ranking is also ignored [<a href="#l2r-intro">, </a>]. Here is a brief explanation of some of widely used pairwise algorithms:

<div class="p"><!----></div>

<ul>
<li> <i>RankNet</i> []: Widely applied by commercial search engines, it uses gradient descent method and neural network to model the underlying ranking function.
<div class="p"><!----></div>
</li>

<li> <i>RankBoost</i> []: It adopts AdaBoost algorithm for the classification over document pairs.
<div class="p"><!----></div>
</li>

<li> <i>LambdaRank</i> []: It considers the evaluation measures to set its pair weight. In particular, the evaluation measures (which are position based) are directly used to define the gradient with respect to each document pair in the training process.
<div class="p"><!----></div>
</li>

<li> <i>LambdaMART</i> []: It combines the strengths of boosted tree classification and <i>LambdaRank</i>.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
The listwise approach takes the entire set of documents associated with a query in the training data as the input and predicts their ground truth labels []. The group structure of ranking is maintained and
ranking evaluation measures can be more directly incorporated into the loss functions in learning [<a href="#l2r-intro">, </a>]. In the following, two common listwise algorithms are briefly discussed:

<div class="p"><!----></div>

<ul>
<li> <i>AdaRank</i> []: It applies the evaluation measures on the framework of Boosting and focuses on effectively optimization.
<div class="p"><!----></div>
</li>

<li> <i>ListNet</i> []: It uses different probability distributions in order to define the loss function.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
As the different data-set are compared in , listwise techniques are in general the most effective among the others. However, the choice of the learning evaluation measure and the rank cutoff may have a noticeable impact on the effectiveness of the learned model [].

<div class="p"><!----></div>
A highly important point in all kinds of information retrieval systems is evaluation of results. The evaluation on the performance of a ranking model is carried out by comparison between the ranking lists output by the model and the ranking lists given as the ground truth [<a href="#l2r-intro">, </a>].Some common IR evaluation methods like Mean average precision (MAP), [Normalized] Discounted Cumulative Gain ([N]DCG), Mean Reciprocal Rank (MRR) are also widely user in leaning to rank evaluation. Among the mentioned metrics, DCG/NDCG is the only one used for graded relevance.

<div class="p"><!----></div>
Recently,  have proposed a new evaluation metrics called Expected Reciprocal Rank (ERR) which claims to model user's satisfaction with search results better than the DCG metric.  addresses the underlying independence assumption of DCG that a document in a given position has always the same gain and discount independently of the documents shown above it. It asserts that based on research on modeling user click behavior [,], the likelihood a user examines the document at rank i is dependent on how satisfied the user was with previously observed documents in the ranked list. In other words, it assumes that a user is more likely to stop browsing if they have already seen one or more highly relevant documents. By presenting ERR formula in ], it claims that results reflects real user browsing behavior better and quantifies user satisfaction more accurately than DCG.


 <a id="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Apply The Methods and Results</h2>
<a id="sec:apply">
</a>
In this section, we applied different approaches on the platform. By comparing the methods, we aim to discover the most appropriate one regarding to the project's characteristics and data.

<div class="p"><!----></div>
     <a id="tth_sEc4.1"></a><h3>
4.1&nbsp;&nbsp;Aggregation Functions</h3>
In order to compare different aggregation functions, similar to <a href="#agg-gp2"> []</a> three algorithms are selected. GP2 as well as Top1 and Top5 which are two common forms of TopN aggregation algorithm. TopN refers to algorithm that summarizes the N top documents (i.e. Top1 only using the top associated document to rank the candidates).

<div class="p"><!----></div>
Feedbacks of proof-readers after every translation are used as a basis for evaluating the algorithms. Since feedbacks are a measure for quality of translation, the more similar the ranking of algorithms to feedbacks are the better it is. 

<div class="p"><!----></div>
In order to calculate the correlation value, we applied Spearman Rank Order and Kendall Rank Correlation as two common methods. Table <a href="#table:correlation">I</a> shows the results of Spearman correlation coefficient (r<sub>s</sub>) and Kendall's tau coefficient (&#964;) using 181 records of purchased orders. Table <a href="#table:signtest">II</a> represents the Significance Test of both methods calculated by R.

<div class="p"><!----></div>
As it is usual in many cases, the coefficient value of Spearman is slightly higher than Kendall's. The outcome shows an approximately weak correlation between aggregation functions and feedbacks of proof-readers. Regarding to P-Value of significance test shown in Table <a href="#table:signtest">II</a>, a meaningful relation between GP2 and feedbacks can be considered. Top1 which has the worst values in the table shows a meaningless and near random correlation though.

<div class="p"><!----></div>
Comparing the algorithms, GP2 outperforms the others in both correlation tests. In comparison to Top1, Top5 has slightly better performance. The results are also nearly the same when comparing based on language-pairs.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a id="tth_tAb1">
</a> 
<div style="text-align:center"><div style="text-align:center">Table 1: Correlation Test Between Algorithms and Proof-readers' Feedbacks</div>
<a id="table:correlation">
</a>
</div>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a id="tth_tAb2">
</a> 
<div style="text-align:center"><div style="text-align:center">Table 2: P-Value of Significance of Correlation Test Algorithms</div>
<a id="table:signtest">
</a>
</div>

<div class="p"><!----></div>
     <a id="tth_sEc4.2"></a><h3>
4.2&nbsp;&nbsp;Learning To Rank</h3>
TODO

<div class="p"><!----></div>

<div class="p"><!----></div>
<a id="tth_tAb3">
</a> 
<div style="text-align:center"><div style="text-align:center">Table 3: Results of Applying Learning to Rank Methods based on NDCG and ERR evaluation measures</div>
<a id="table:l2rresult">
</a>
</div>

<div class="p"><!----></div>
 <a id="tth_sEc5"></a><h2>
5&nbsp;&nbsp;Conclusion</h2>
<a id="sec:conclusion">
</a>
TODO

<div class="p"><!----></div>

<h2>References</h2>

<dl>
 <dt><a href="#CITEer" id="er">[ ]</a></dt><dd>
Balog, Krisztian, Yi&nbsp;Fang, Maarten de&nbsp;Rijke, Pavel Serdyukov, and Luo Si.
 <em>Expertise Retrieval - Foundations and Trends in Information
  Retrieval 6</em>, pages 127-256.
 Number 2-3. 2012.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEer-sparse" id="er-sparse">[ ]</a></dt><dd>
K.&nbsp;Balog, T.&nbsp;Bogers, L.&nbsp;Azzopardi, M.&nbsp;de&nbsp;Rijke, and A.&nbsp;van&nbsp;den Bosch.
 Broad expertise retrieval in sparse data environments.
 <em>SIGIR</em>, 2007.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEtrec2005" id="trec2005">[ ]</a></dt><dd>
Y.&nbsp;Cao, J.&nbsp;Liu, S.&nbsp;Bao, and H.&nbsp;Li.
 Research on expert search at enterprise track of trec 2005.
 <em>TREC</em>, 2005.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEagg-gp2" id="agg-gp2">[ ]</a></dt><dd>
Cummins, Ronan, Mounia Lalmas, and Colm O'Riordan.
 Learning aggregation functions for expert search.
 <em>ECAI</em>, 2010.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEer-community-aware" id="er-community-aware">[ ]</a></dt><dd>
Deng, Hongbo, Irwin King, and Michael&nbsp;R. Lyu.
 Enhanced models for expertise retrieval using community-aware
  strategies.
 <em>Systems Man and Cybernetics Part B: Cybernetics IEEE
  Transactions</em>, 2012.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEl2r-intro" id="l2r-intro">[ ]</a></dt><dd>
Hang&nbsp;L. I.
 A short introduction to learning to rank.
 <em>IEICE TRANSACTIONS on Information and Systems</em>, 2011.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEagg-vote" id="agg-vote">[ ]</a></dt><dd>
Macdonald, Craig, and Iadh Ounis.
 Voting for candidates: adapting data fusion techniques for an expert
  search task.
 <em>the 15th ACM international conference on Information and
  knowledge management</em>, 2006.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEagg-learning" id="agg-learning">[ ]</a></dt><dd>
Macdonald, Craig, and Iadh Ounis.
 Learning models for ranking aggregates.
 <em>Advances in Information Retrieval. Springer Berlin Heidelberg</em>,
  2011.</dd>
</dl>


<div class="p"><!----></div>

<br /><br /><hr /><small>File translated from
T<sub><span class="small">E</span></sub>X
by <a href="http://hutchinson.belmont.ma.us/tth/">
T<sub><span class="small">T</span></sub>H</a>,
version 4.03.<br />On 21 Feb 2014, 09:10.</small>
</div></body></html>
