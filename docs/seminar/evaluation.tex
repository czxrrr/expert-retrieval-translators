\section{Apply The Methods and Results}
\label{sec:apply}
In this section, we applied different approaches on the platform. By comparing the methods, we aim to discover the most appropriate one regarding to the project's characteristics and data.

\subsection{Aggregation Functions}
In order to compare different aggregation functions, similar to \citet{agg-gp2} three algorithms are selected. $GP2$ as well as $Top1$ and $Top5$ which are two common forms of $TopN$ aggregation algorithm. $TopN$ refers to algorithm that summarizes the $N$ top documents (i.e. $Top1$ only using the top associated document to rank the candidates).

Feedbacks of proof-readers after every translation are used as a basis for evaluating the algorithms. Since feedbacks are a measure for quality of translation, the more similar the ranking of algorithms to feedbacks are the better it is. 

By calculating the value of each mentioned algorithm on all finished translations, we achieve three lists of translations each annotated with an additional aggregation value field. In the next step, in order to compare the algorithms, we test the correlation of each list with the proof-readers' feedbacks.

In order to calculate the correlation value, we applied Spearman Rank Order and Kendall Rank Correlation as two common methods. Table \ref{table:correlation} shows the results of Spearman correlation coefficient ($r_s$) and Kendall's tau coefficient ($\tau$) using $181$ records of purchased orders. In addition, it represents the Significance Test of both methods. For Spearman's test, p-values are computed using algorithm AS 89 \citep{as89}.

As it is usual in many cases, the coefficient value of Spearman is slightly higher than Kendall's. The outcome shows an approximately weak correlation between aggregation functions and feedbacks of proof-readers. Regarding to P-Value of significance test shown in Table \ref{table:correlation}, a meaningful relation between GP2 and feedbacks can be considered. Top1 which has the worst values in the table shows a meaningless and near random correlation though.

Comparing the algorithms, $GP2$ outperforms the others in both correlation tests. In comparison to $Top1$, $Top5$ has slightly better performance. The results are also nearly the same when comparing based on language-pairs.

\begin{table}
\begin{center}{
\begin{tabular}{c c|c|c|c|}
\cline{3-5} & & Top1 & Top5 & GP2  \\
\hline \multicolumn{1}{ |c| }{\multirow{2}{*}{$r_s$}} & Correlation Test & 0.052 & 0.089 & 0.145\\
\cline{2-5} \multicolumn{1}{ |c| }{} & P-Value & 0.4866 & 0.2295 & 0.05038\\
\hline \multicolumn{1}{ |c| }{\multirow{2}{*}{$\tau$}} & Correlation Test & 0.034 & 0.059 & 0.102\\
\cline{2-5} \multicolumn{1}{ |c| }{} & P-Value & 0.5157 & 0.2562 & 0.05263\\
\hline
\end{tabular}
}
\caption{Correlation Test Between Algorithms and Proof-readers' Feedbacks as well as P-Value of Significance of Correlation Test}
\label{table:correlation}
\end{center}
\end{table}

\subsection{Learning To Rank}
In order to accumulate data required for ranking model, we conduct a survey filled by eight human annotators. The questions of survey represent three translators each with four criteria (price, delivery time, proficiency and number of cooperation times). The annotators rate the questions from one to three based on \textit{Total Order} strategy. The accumulated data consists of $400$ annotated list and overall $1200$ records.

We use the RankLib library \footnote{http://sourceforge.net/p/lemur/wiki/RankLib} to apply Learning to Rank methods. The library provides the  implementation of some Learning to Rank algorithms as well as evaluation measures in Java. By splitting the data in train, validation and test datasets, we use 5-Fold Cross-Validation on each run. One pointwise, three pairwise and two listwise methods are applied.

Since evaluation of the results should be applied on whole the result list (with $3$ items), we have to use relevance grading evaluation measures like DCG, NDCG or ERR. In the current study, NDGC and ERR both with rank position at $3$ are used.

Because of the short final predicted list ($3$ items), every evaluator measure returns high result value even for a random ranker. This issue urges the need to run the evaluation on random data in order to find a baseline for comparison. We tackle the problem with two approaches. In the first approach, we run all the algorithms on data with random generated labels. In order to increase the accuracy of results, the process of generating randomized label is repeated $5$ times. The second approach is developing a simple ranker which randomly predicts the rank of each record. In fact, the first tests whether there is anything to learn in the data and the second examines if the learning algorithms learn from the data.

The result is shown in Table \ref{table:l2rresult}.  Figure \ref{fig:l2rdiagram} depicts the corresponding data in diagrams. As it is shown, random values define a base line for comparison the goodness of methods. Among the applied methods, Linear Regression and LambdaMART tend to have better results. Furthermore, tracing NDCG and ERR diagrams shows a considerable similarity in behavior of both evaluation methods regarding to the data.

In addition to comparing the methods, features comparison can be an interesting point. Table \ref{table:l2rcoefficient} shows the coefficients of features, calculated in Linear Regression model. The coefficient value is a measure for understanding the importance of each feature in comparison to the others. 

As it is shown, price and delivery time seem to be the most effective features while the number of cooperation times has the lowest importance. Surprisingly, proficiency plays a small role in the ranking of the translators. It can be because of the proposed business plan of the platform to the clients which guarantees an acceptable quality of translation. In addition, we expect that by applying the methods on much more amount of data, the proficiency feature gains more importance.

\begin{table}
\begin{center}{
\begin{tabular}{|c|c|c|c|c|}
\hline \multirow{2}{*}{Method} & \multicolumn{2}{ c| }{NDCG@3} & \multicolumn{2}{ c| }{ERR@3}  \\
\cline{2-5} & Result & Random & Result & Random \\
\hline Linear Regression & \textbf{0.935} & 0.833 & \textbf{0.451} & 0.375 \\
\hline RankNet & 0.876 & 0.834 & 0.394 & 0.378 \\
\hline RankBoost & 0.909 & 0.831 & 0.432 & 0.374 \\
\hline LambdaMART & 0.93 & 0.832 & 0.447 & 0.373 \\
\hline ListNet & 0.915 & 0.831 & 0.439 & 0.375 \\
\hline AdaRank & 0.857 & 0.83 & 0.399 & 0.373 \\
\hline Random Ranker & 0.832 & 0.832 & 0.375 & 0.378 \\
\hline
\end{tabular}
}
\caption{Results of applying Learning to Rank methods based on NDCG and ERR evaluation measures}
\label{table:l2rresult}
\end{center}
\end{table}


\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{figures/l2rdiagram.png}
\caption{Learning to Rank Results
\label{fig:l2rdiagram}}
\end{center}
\end{figure}

\begin{table}
\begin{center}{
\begin{tabular}{|c|c|}
\hline Feature & Value  \\
\hline Price & 2.002 \\
\hline Duration & 0.057 \\
\hline Proficiency & -0.048 \\
\hline Number of Cooperation Times & -0.313 \\
\hline
\end{tabular}
}
\caption{Coefficient value of features in Linear Regression model}
\label{table:l2rcoefficient}
\end{center}
\end{table}
