\section{Apply The Methods and Results}
\label{sec:apply}
In this section, we applied different approaches on the platform. By comparing the methods, we aim to discover the most appropriate one regarding to the project's characteristics and data.

\subsection{Aggregation Functions}
In order to compare different aggregation functions, similar to \citet{agg-gp2} three algorithms are selected. $GP2$ as well as $Top1$ and $Top5$ which are two common forms of $TopN$ aggregation algorithm. $TopN$ refers to algorithm that summarizes the $N$ top documents (i.e. $Top1$ only using the top associated document to rank the candidates).

Feedbacks of proof-readers after every translation are used as a basis for evaluating the algorithms. Since feedbacks are a measure for quality of translation, the more similar the ranking of algorithms to feedbacks are the better it is. 

In order to calculate the correlation value, we applied Spearman Rank Order and Kendall Rank Correlation as two common methods. Table \ref{table:correlation} shows the results of Spearman correlation coefficient ($r_s$) and Kendall's tau coefficient ($\tau$) using $181$ records of purchased orders. Table \ref{table:signtest} represents the Significance Test of both methods calculated by R.

As it is usual in many cases, the coefficient value of Spearman is slightly higher than Kendall's. The outcome shows an approximately weak correlation between aggregation functions and feedbacks of proof-readers. Regarding to P-Value of significance test shown in Table \ref{table:signtest}, a meaningful relation between GP2 and feedbacks can be considered. Top1 which has the worst values in the table shows a meaningless and near random correlation though.

Comparing the algorithms, $GP2$ outperforms the others in both correlation tests. In comparison to $Top1$, $Top5$ has slightly better performance. The results are also nearly the same when comparing based on language-pairs.

\begin{table}
\begin{center}
\scalebox{1.2}{
\begin{tabular}{p{1cm}|c|c|c|}
\cline{2-4} & Top1 & Top5 & GP2  \\
\hline \multicolumn{1}{ |c| }{$r_s$} & 0.052 & 0.089 & 0.145\\
\hline \multicolumn{1}{ |c| }{$\tau$} & 0.034 & 0.059 & 0.102\\
\hline
\end{tabular}
}
\caption{Correlation Test Between Algorithms and Proof-readers' Feedbacks}
\label{table:correlation}
\end{center}
\end{table}

\begin{table}
\begin{center}
\scalebox{1.2}{
\begin{tabular}{p{1cm}|c|c|c|}
\cline{2-4} & Top1 & Top5 & GP2  \\
\hline \multicolumn{1}{ |c| }{$r_s$} & 0.4866 & 0.2295 & 0.05038\\
\hline \multicolumn{1}{ |c| }{$\tau$} & 0.5157 & 0.2562 & 0.05263\\
\hline
\end{tabular}
}
\caption{P-Value of Significance of Correlation Test Algorithms}
\label{table:signtest}
\end{center}
\end{table}

\subsection{Learning To Rank}
In order to accumulate required data to achieve translators' ranking model, we conduct a survey using eight human annotators. Each question of survey represents a situation in which three translators each with four criteria are proposed. Based on \textit{total order} strategy, annotators should rate them from one to three. The accumulated data has $400$ situations consists of $1200$ annotations.

In order to apply Learning to Rank algorithms on data, we use RankLib library \footnote{http://sourceforge.net/p/lemur/wiki/RankLib}. Each method is calculated using 10-Fold Cross-Validation by splitting the data in train, validation and test datasets. We select $5$ learning to rank methods, three pairwise and two listwise. 

In order to evaluate the results, NDCG and ERR measures are selected. 

In order to evaluate the results, we need to choose an evaluation measure which applies on whole the list (with $3$ items) and not a portion of that. Therefore just relevance grading evaluation measures like DCG, NDCG and ERR can be used. In current study, we use NDGC and ERR both with rank position at $3$.

TODO discussion about the results.

\begin{table}
\begin{center}
\scalebox{1.2}{
\begin{tabular}{|c|c|c|c|c|}
\hline \multirow{2}{*}{Method} & \multicolumn{2}{ c| }{NDCG@3} & \multicolumn{2}{ c| }{ERR@3}  \\
\cline{2-5} & Result & Random & Result & Random \\
\hline Random Ranker & 0.832 & 0.832 & 0.375 & 0.378 \\
\hline RankNet & 0.876 & 0.834 & 0.394 & 0.378 \\
\hline RankBoost & 0.909 & 0.831 & 0.432 & 0.374 \\
\hline LambdaMART & \textbf{0.93} & 0.832 & \textbf{0.447} & 0.373 \\
\hline AdaRank & 0.857 & 0.83 & 0.399 & 0.373 \\
\hline ListNet & 0.915 & 0.831 & 0.439 & 0.375 \\
\hline
\end{tabular}
}
\caption{Results of Applying Learning to Rank Methods based on NDCG and ERR evaluation measures}
\label{table:l2rresult}
\end{center}
\end{table}


\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{figures/l2rdiagram.png}
\caption{Learning to Rank Results
\label{fig:l2rdiagram}}
\end{center}
\end{figure}

