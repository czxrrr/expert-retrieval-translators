\section{Experimental Results}
\label{sec:apply}
In this section, we applied the different approaches presented in the previous section within our platform. By comparing the methods, we aim to discover the most appropriate one considering to the project's characteristics and data. 

\subsection{Aggregation Functions}
Translator's proficiency on the topic of the query is one of the attributes used for creating the ranking model. In order to obtain the value, we aggregate the similarity values of the previously translated documents with the query document. Selecting the most appropriate aggregation function is the objective of the current section.

In order to evaluate aggregation functions results, we use the assessment of the proof-readers after every translation. As mentioned before, the hypothesis is that a translator is likely to do a better job if she is familiar with the vocabulary of the query. We can then use the proof-readers assessments as a gold standard and observe which of the three aggregation methods correlates more with the scores given by the proof-readers. 

Similar ~\cite{agg-gp2}, we compare three aggregation algorithms: $GP2$, $Top5$ and $Top1$. $GP2$ is the state of the art, while $Top1$ and $Top5$ are two common forms of $TopN$ aggregation algorithm which refers to algorithm that summarizes the $N$ top documents. Furthermore, $Top1$ can be interpreted as the maximum similarity score, and follows the intuition that it is sufficient for a translator to have had only one highly similar translation job before in order to do a good job on the new task.

As input data we have $181$ translation orders collected from the live system. For each of them, we know the translator who performed the translation (including all her previously translated documents), the text of the document to be translated, and the score given by the proof-reader. Because this is historical data, evaluating the aggregation method cannot be done by giving the same translation task to different translators. However, by calculating the value of each mentioned algorithm on all finished translations, we achieve three lists of translations each annotated with an additional aggregation value field. Then, in order to compare the algorithms, we test the correlation of each list with the list of translations annotated with proof-readers' assessments. Again, the assumption being that if we rank translation jobs by aggregation scores, the top elements would have high proof-reader scores, while the bottom elements lower scores. The distribution of data between the values of each aggregation function and the proof-reader's assessments is shown in Figure~\ref{fig:aggregationplot}.

\begin{figure}
\centering
\begin{subfigure}{.3\textwidth}
  \centering
	\includegraphics[width=0.99\textwidth]{figures/aggregation_plots_gp2.png}
	\caption{GP2}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
	\includegraphics[width=0.99\textwidth]{figures/aggregation_plots_top5.png}
	\caption{TOP5}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
	\includegraphics[width=0.99\textwidth]{figures/aggregation_plots_top1.png}
	\caption{TOP1}
\end{subfigure}
\vspace{-0.3cm}
\caption{Data distribution of aggregation values against proof-readers' assessments. Axes X shows the assessments of proof-readers. axes Y represents the values of the corresponding aggregation function}
\label{fig:aggregationplot}
\vspace{-0.3cm}
\end{figure}

In order to calculate the correlation value, we applied Spearman Rank Order and Kendall Rank Correlation as two common methods. Table~\ref{table:correlation} shows the results of Spearman correlation coefficient ($r_s$) and Kendall's tau coefficient ($\tau$) using the $181$ records of purchased orders. In addition, it represents the Significance Test of both methods. For Spearman's test, p-values are computed using algorithm AS 89~\cite{as89}.

The coefficient value of Spearman is slightly higher than Kendall's. The outcome shows an approximately weak correlation between aggregation functions and feedbacks of proof-readers. Regarding to p-Value of significance test shown in Table~\ref{table:correlation}, a meaningful relation between $GP2$ and feedbacks can be considered. $Top1$ which has the worst values in the table shows a meaningless and near random correlation though.

Comparing the algorithms, $GP2$ outperforms the others in both correlation tests. In comparison to $Top1$, $Top5$ has slightly better performance. The results are also nearly the same when comparing based on language-pairs.

\begin{table}
\vspace{-0.3cm}
\centering
\begin{tabular}{c c|c|c|c|}
\cline{3-5} & & Top1 & Top5 & GP2  \\
\hline \multicolumn{1}{ |c| }{\multirow{2}{*}{$r_s$}} & Correlation Test & 0.052 & 0.089 & 0.145\\
\cline{2-5} \multicolumn{1}{ |c| }{} & p-Value & 0.4866 & 0.2295 & 0.05038\\
\hline \multicolumn{1}{ |c| }{\multirow{2}{*}{$\tau$}} & Correlation Test & 0.034 & 0.059 & 0.102\\
\cline{2-5} \multicolumn{1}{ |c| }{} & p-Value & 0.5157 & 0.2562 & 0.05263\\
\hline
\end{tabular}
\caption{Correlation Test Between Algorithms and Proof-readers' Feedbacks as well as P-Value of Significance of Correlation Test}
\label{table:correlation}
\end{table}

\subsection{Learning To Rank}
In order to accumulate data required for ranking model, we conduct a survey with eight human annotators. The questions of survey represent three translators each with four criteria (price, delivery time, proficiency and number of cooperation times). As mentioned before, in order to prevent bias in results, the name and the picture of the translators are hidden from the annotators. The annotators rate the questions from one to three based on \textit{Total Order} strategy. The accumulated data consists of $400$ annotated list and overall $1200$ records.

We use the RankLib library\footnote{http://sourceforge.net/p/lemur/wiki/RankLib} to apply a large set of Learning to Rank methods. The library provides the  implementation of some Learning to Rank algorithms as well as evaluation measures in Java. By splitting the data in train, validation and test datasets, we use 5-Fold Cross-Validation on each run. One pointwise, three pairwise and two listwise methods are tested.

Since evaluation of the results should be applied on  the entire result list (with $3$ items), we have to use relevance grading evaluation measures like DCG, NDCG or ERR. In the current study, NDGC and ERR both with rank position at $3$ are used.

Because of the short final predicted list ($3$ items), every  measure returns a very high score. In order to understand the real benefit of our framework compared with just presenting the un-ranked set of three translators, we evaluate two random approaches. In the first approach, we run all the algorithms on data with random generated labels. In order to increase the accuracy of results, the process of generating randomized label is repeated $5$ times. The second approach is developing a simple ranker which randomly predicts the rank of each record. In fact, the first tests whether there is anything to learn in the data and the second examines if the learning algorithms learn from the data.

The result is shown in Table~\ref{table:l2rresult}.  Figure~\ref{fig:l2rdiagram} depicts the corresponding data in diagrams. As it is shown, random values define a base line for comparison the goodness of methods. Among the applied methods, Linear Regression and LambdaMART tend to have better results. Furthermore, tracing NDCG and ERR diagrams shows a considerable similarity in behavior of both evaluation methods regarding to the data.

In addition to comparing the methods, features comparison can be an interesting point. Table~\ref{table:l2rcoefficient} shows the coefficients of features, calculated in Linear Regression model. The coefficient value is a measure for understanding the importance of each feature in comparison to the others. 

As it is shown, price and delivery time seem to be the most effective features while the number of cooperation times has the lowest importance. Surprisingly, proficiency plays a small role in the ranking of the translators. It can be because of the proposed business plan of the platform to the clients which guarantees an acceptable quality of translation. In addition, we expect that by applying the methods on much more amount of data, the proficiency feature gains more importance.

\begin{table}
\vspace{-0.3cm}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline \multirow{2}{*}{Method} & \multicolumn{2}{ c| }{NDCG@3} & \multicolumn{2}{ c| }{ERR@3}  \\
\cline{2-5} & Result & Random & Result & Random \\
\hline Linear Regression & \textbf{0.935} & 0.833 & \textbf{0.451} & 0.375 \\
\hline RankNet & 0.876 & 0.834 & 0.394 & 0.378 \\
\hline RankBoost & 0.909 & 0.831 & 0.432 & 0.374 \\
\hline LambdaMART & 0.93 & 0.832 & 0.447 & 0.373 \\
\hline ListNet & 0.915 & 0.831 & 0.439 & 0.375 \\
\hline AdaRank & 0.857 & 0.83 & 0.399 & 0.373 \\
\hline Random Ranker & 0.832 & 0.832 & 0.375 & 0.378 \\
\hline
\end{tabular}
\caption{Results of applying Learning to Rank methods based on NDCG and ERR evaluation measures}
\label{table:l2rresult}
\end{table}


\begin{figure}
\centering
\includegraphics[scale=0.6]{figures/l2rdiagram.png}
\caption{Learning to Rank Results}
\label{fig:l2rdiagram}
\end{figure}

\begin{table}
\vspace{-0.3cm}
\centering
\begin{tabular}{|c|c|}
\hline Feature & Value  \\
\hline Price & 2.002 \\
\hline Duration & 0.057 \\
\hline Proficiency & -0.048 \\
\hline Number of Cooperation Times & -0.313 \\
\hline
\end{tabular}
\caption{Coefficient value of features in Linear Regression model}
\label{table:l2rcoefficient}
\end{table}
