\section{Methods and Related Work}
\label{sec:methods}
With the development of information retrieval (IR) techniques, many research efforts go beyond traditional document retrieval and address high-level IR such as entity retrieval and expertise retrieval~\cite{er-sparse}. The goal of expertise retrieval is to link humans to expertise areas, and vice versa. In other words, the task of expertise retrieval is to identify a set of persons with relevant expertise for the given query~\cite{er,er-community-aware}.
The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, following by rapid progress being made in terms of modeling, algorithms, and evaluation aspects~\cite{trec2005,er-community-aware}.

Cao and colleagues~\cite{trec2005} propose two principal approaches  in the expertise retrieval area based on probabilistic language modeling techniques. They were formalized as so-called \textit{candidate models} and \textit{document models}. The candidate-based approach, also referred to as profile-based method, builds a textual representation of candidate experts and then ranks them based on the query. The document models first find documents relevant to the topic and then locate the experts associated with these documents~\cite{er}.

In either of the two models, aggregation functions have a significant effect on the performance of expert retrieval systems. Aggregate tasks are those where documents' similarities are not
the final outcome, but instead an intermediary component. In expert search, a ranking of candidate persons with relevant expertise to a query is generated after aggregation of their related documents~\cite{agg-learning}.

Ranking techniques are an essential part of each IR framework. In recent years, Learning to Rank (L2R) has been studied extensively especially for document retrieval. It refers to machine learning techniques for training the model in a ranking task~\cite{er}. In essence, expert search is a ranking problem and thus the existing L2R techniques can be naturally applied to it~\cite{l2r-intro}.

For the task at hand, we found that the two methods have to be used in two different steps. Aggregation can be used to bring together in one value elements essentially of the same nature. In this case - query similarity scores of different documents. The test collection at hand relies for its topical similarity exclusively on term frequencies, as there are no hyperlinks, metadata, or other sources of information in the documents. As a second step, in order to bring in attributes orthogonal to topical similarity, learning to rank methods are an obvious choice. In the following, we describe related work related to these two aspects, and in doing so prepare the ground for our experiments, which we present in the next section.

\subsection{Aggregation Functions}
The aggregation function has a significant impact on the performance of Expert Retrieval system~\cite{agg-gp2}. As a usual scenario in expert retrieval systems, first each document related to an expert is scored and ranked regarding to query. Then, the top $N$ document scores associated with a candidate expert are aggregated in order to rank the experts. 

MacDonald and Ounis~\cite{agg-vote} consider expert search as a voting problem, where documents vote for the candidates with relevant expertise. Eleven data fusion methods as well as three statistically different document weighting models were tested in their experiments. In practice, the approach considers both the number of documents and expert features regarding to the ranking score of the documents. The results show that while some of adapted voting techniques most likely outperform others, the proposed approach is effective when using the appropriate one.

Cummins and colleagues~\cite{agg-gp2}  study the effect of different features on the aggregation function. They show that the number of documents is an important factor, in that the performance of different queries are optimal for different values of $N$. Comparing query-based features using statistical measures, they infer that the document features (such as TF, IDF) may not, in general, be able to predict the optimal number of documents to aggregate for each query. In contrast, individual Expert Features have been shown to be more informative such that relevant experts are associated with a higher ranked document than non-relevant experts. More interestingly, relevant experts are associated with less documents on average.

Focusing on these features Cummins et al.~\cite{agg-gp2} introduce a new aggregation method. It uses genetic programming to learn a formula for the weights of document associations within the candidate profiles. The formula, denoted as $GP2$, is as follows:

\begin{align*}
GP2 = \frac{\sqrt{\sqrt{2/{no\_docs_x}_i}}/(\sqrt{(10/R)+R})}{\sqrt{sq(10/R)+R+sq(10/R)+\sqrt{R*2}}}
\end{align*}

\noindent 
where $R$ is the rank of the document in the initial ranking and ${no\_docs_x}_i$ is the total number of documents associated with expert $x_i$.

\subsection{Learning To Rank}
Learning to rank refers to machine learning techniques for training a model in a ranking task. Due to importance of ranking problems, learning to rank has been drawing broad attention in the machine learning community recently. 

In the learning to rank approach, the ranking problem is transformed to classification, regression and ordinal classification, and existing methods and techniques for solving machine learning problems are applied. As Hang~\cite{l2r-intro} points out, the relation between learning to rank and ordinal classification is that, in ranking, one cares more about accurate ordering of objects, while in ordinal classification, one cares more about accurate ordered-categorization of objects.

One primitive step in accumulating data required for learning to rank, is relevance judgments, normally done by human annotators. Lie~\cite{l2r-book} presents the three main strategies in learning to rank:
\begin{itemize}
\item \textit{Relevance degree}: In this method, the annotator specifies whether an object is relevant or not to the query. It can be either in binary judgment or by specifying the degree of relevance (e.g., Perfect, Excellent, Good, Fair, or Bad).
\item \textit{Pairwise preference}: The annotator compares a pair of objects in order to specify which one is more relevant with regards to a query.
\item \textit{Total order}: The annotator specifies the total order of all objects with respect to a query by rating each object.
\end{itemize}

Among the three mentioned kinds of judgments, the first one is the most popularly used judgment since is the easiest to obtain. While the third one is more accurate but laborious for human annotators.

The learning to rank techniques are categorized in three main groups: \textit{Pointwise}, \textit{Pairwise} and \textit{Listwise}.

In the pointwise approach, the ranking problem is transformed to classification, regression or ordinal classification. Therefore, the group structure of ranking is ignored in this approach~\cite{l2r-intro}. Here, linear or polynomial regression are widely used methods.

The pairwise approach transforms the ranking problem into pairwise classification or regression. In fact, it cares about the relative order between two documents. Similar to the pointwise approach, the pairwise method also ignores the group structure of ranking~\cite{l2r-intro}. Here is a brief explanation of some pairwise algorithms:

\begin{itemize}
\item \textit{RankNet}~\cite{l2r-ranknet}: Widely applied by commercial search engines, it uses gradient descent method and neural network to model the underlying ranking function.
\item \textit{RankBoost}~\cite{l2r-rankboost}: It adopts AdaBoost algorithm for the classification over the object pairs.
\item \textit{LambdaRank}~\cite{l2r-lambdarank}: It considers the evaluation measures to set its pair weight. In particular, the evaluation measures (which are position based) are directly used to define the gradient with respect to each document pair in the training process.
\item \textit{LambdaMART}~\cite{l2r-lambdamart}: It combines the strengths of boosted tree classification and LambdaRank.
\end{itemize}

The listwise approach takes the entire set of documents associated with a query in the training data as the input and predicts their ground truth labels~\cite{l2r-book}. In contradiction to two previous approaches, it maintains the group structure of ranking. In addition, ranking evaluation measures can be more directly incorporated into the loss functions in learning~\cite{l2r-intro}. In the following, two common listwise algorithms are briefly discussed:

\begin{itemize}
\item \textit{AdaRank}~\cite{l2r-adarank}: It applies the evaluation measures on the framework of Boosting and focuses on effectively optimization.
\item \textit{ListNet}~\cite{l2r-listnet}: It uses different probability distributions in order to define the loss function.
\end{itemize}

Lie~\cite{l2r-book} compares the algorithms by applying on different data-sets. It concludes that listwise techniques are in general the most effective among the others. However, the choice of the learning evaluation measure and the rank cutoff may have a noticeable impact on the effectiveness of the learned model~\cite{l2r-when}.

\subsection{Evaluation}

A critical point in all kinds of information retrieval systems is the evaluation of results. The evaluation on the performance of a ranking model is carried out by comparison between the ranking lists output of the model and the ranking lists given as the ground truth. Some common IR evaluation methods like Mean average precision (MAP), [Normalized] Discounted Cumulative Gain ([N]DCG), Mean Reciprocal Rank (MRR) are also widely user in leaning to rank evaluation. Among the mentioned metrics, DCG/NDCG is the only one used for graded relevance.

Recently, Chapelle and Zhang~\cite{l2r-err} have proposed Expected Reciprocal Rank (ERR) which claims to model user's satisfaction with search results better than the DCG metric. Their work addresses the underlying independence assumption of DCG that a document in a given position has always the same gain and discount independently of the documents shown above it. It asserts that based on research on modeling user click behavior~\cite{l2r-clickmodel1,l2r-clickmodel2}, the likelihood a user examines the document at rank $i$ is dependent on how satisfied the user was with previously observed documents in the ranked list. In other words, it assumes that a user is more likely to stop browsing if they have already seen one or more highly relevant documents. Introducing the ERR formula, Chapelle and Zhang claim that results reflect real user browsing behavior better and quantifies user satisfaction more accurately than DCG.
