\section{Methods and Related Work}
\label{sec:methods}
In this section, we study different methods and algorithms regarding to Aggregation Functions and Learning to Rank.

\subsection{Aggregation Functions}
The aggregation function has a significant impact on the performance of Expert Retrieval system. As a usual scenario in expert retrieval systems, first each document related to an expert is scored and ranked regarding to query. Then, the top $N$ document scores associated with a candidate expert are aggregated in order to rank the experts. 

The effect of different features on the aggregation function is studied in \cite{agg-gp2}. They show that the number of documents is an important factor, such that the performance of different queries are optimal for different values of $N$. Comparing query-based features using statistical measures, they infer that the document features (such as TF, IDF) may not, in general, be able to predict the optimal number of documents to aggregate for each query. In contrast, individual Expert Features have been shown to be more informative such that relevant experts are associated with a higher ranked document than non-relevant experts. More interestingly, relevant experts are associated with less documents on average.

\cite{agg-vote} looks to expert search as a voting problem, where documents vote for the candidates with relevant expertise. Eleven data fusion methods as well as three statistically different document weighting models were tested. In practice, the approach considers both the number of documents and expert features regarding to the ranking score of the documents. The results show that while some of adapted voting techniques most likely outperform others, the proposed approach is effective when using the appropriate one.

Later on, focusing on related features discussed before \cite{agg-gp2} introduces a new aggregation method. It uses genetic programming to learn a formula for the weights of document associations within the candidate profiles. The formula denoted as $GP2$ is as follows:

\begin{align*}
GP2 = \frac{\sqrt{\sqrt{2/{no\_docs_x}_i}}/(\sqrt{(10/R)+R})}{\sqrt{sq(10/R)+R+sq(10/R)+\sqrt{R*2}}}
\end{align*}

\noindent 
where $R$ is the rank of the document in the initial ranking and ${no\_docs_x}_i$ is the total number of documents associated with expert $x_i$.

\subsection{Learning To Rank}
Learning to rank refers to machine learning techniques for training the model in a ranking task. Due to importance of ranking problems, learning to rank has been drawing broad attention in the machine learning community recently. 

In the learning to rank approach, the ranking problem is transformed to classification, regression and ordinal classification, and existing methods and techniques for solving machine learning problems are applied. As \cite{l2r-intro} points out, the relation between learning to rank and ordinal classification is that, in ranking, one cares more about accurate ordering of objects, while in ordinal classification, one cares more about accurate ordered-categorization of objects.

One primitive step in accumulating data required for learning to rank, is relevance judgment normally done by human annotators. \cite{l2r-book} introduces three main strategies as follows.
\begin{itemize}
\item \textit{Relevance degree}: In this method, the annotator specifies whether an object is relevant or not to the query. It can be either in binary judgment or by specifying the degree of relevance (e.g., Perfect, Excellent, Good, Fair, or Bad).
\item \textit{Pairwise preference}: The annotator compares a pair of objects in order to specify which one is more relevant with regards to a query.
\item \textit{Total order}: The annotator specifies the total order of all objects with respect to a query by rating each object.
\end{itemize}

Among the three mentioned kinds of judgments, the first one is the most popularly used judgment since is the easiest to obtain. While the third one is more accurate but laborious for human annotators.

The learning to rank techniques are categorized in three main groups: \textit{Pointwise}, \textit{Pairwise} and \textit{Listwise}.

In the pointwise approach, the ranking problem is transformed to classification, regression or ordinal classification. Therefore, the group structure of ranking is ignored in this approach \cite{l2r-intro}. Linear or polynomial regression are widely used methods regarding to the approach.

The pairwise approach transforms the ranking problem into pairwise classification or regression. In fact, it cares about the relative order between two documents. Similar to the pointwise, the pairwise approach also ignores the group structure of ranking\cite{l2r-intro}. Here is a brief explanation of some pairwise algorithms:

\begin{itemize}
\item \textit{RankNet} \cite{l2r-ranknet}: Widely applied by commercial search engines, it uses gradient descent method and neural network to model the underlying ranking function.
\item \textit{RankBoost} \cite{l2r-rankboost}: It adopts AdaBoost algorithm for the classification over the object pairs.
\item \textit{LambdaRank} \cite{l2r-lambdarank}: It considers the evaluation measures to set its pair weight. In particular, the evaluation measures (which are position based) are directly used to define the gradient with respect to each document pair in the training process.
\item \textit{LambdaMART} \cite{l2r-lambdamart}: It combines the strengths of boosted tree classification and LambdaRank.
\end{itemize}

The listwise approach takes the entire set of documents associated with a query in the training data as the input and predicts their ground truth labels \cite{l2r-book}. In contradiction to two previous approaches, it maintains the group structure of ranking. In addition, ranking evaluation measures can be more directly incorporated into the loss functions in learning \cite{l2r-intro}. In the following, two common listwise algorithms are briefly discussed:

\begin{itemize}
\item \textit{AdaRank} \cite{l2r-adarank}: It applies the evaluation measures on the framework of Boosting and focuses on effectively optimization.
\item \textit{ListNet} \cite{l2r-listnet}: It uses different probability distributions in order to define the loss function.
\end{itemize}

\cite{l2r-book} compares the algorithms by applying on different data-sets. It concludes that listwise techniques are in general the most effective among the others. However, the choice of the learning evaluation measure and the rank cutoff may have a noticeable impact on the effectiveness of the learned model \cite{l2r-when}.

A critical point in all kinds of information retrieval systems is the evaluation of results. The evaluation on the performance of a ranking model is carried out by comparison between the ranking lists output of the model and the ranking lists given as the ground truth. Some common IR evaluation methods like Mean average precision (MAP), [Normalized] Discounted Cumulative Gain ([N]DCG), Mean Reciprocal Rank (MRR) are also widely user in leaning to rank evaluation. Among the mentioned metrics, DCG/NDCG is the only one used for graded relevance.

Recently, \cite{l2r-err} have proposed a new evaluation metrics called Expected Reciprocal Rank (ERR) which claims to model user's satisfaction with search results better than the DCG metric. \cite{l2r-err} addresses the underlying independence assumption of DCG that a document in a given position has always the same gain and discount independently of the documents shown above it. It asserts that based on research on modeling user click behavior \cite{l2r-clickmodel1,l2r-clickmodel2}, the likelihood a user examines the document at rank $i$ is dependent on how satisfied the user was with previously observed documents in the ranked list. In other words, it assumes that a user is more likely to stop browsing if they have already seen one or more highly relevant documents. Introducing the ERR formula, \cite{l2r-err} claim that results reflect real user browsing behavior better and quantifies user satisfaction more accurately than DCG.
